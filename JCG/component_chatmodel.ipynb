{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] =\"https://api.zhizengzeng.com/v1/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set ephemeral cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# set up model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.7 ms, sys: 1.92 ms, total: 20.6 ms\n",
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-283b3df2-06a1-4cd6-83c5-f91cfb5371be-0', usage_metadata={'input_tokens': 11, 'output_tokens': 18, 'total_tokens': 29})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# the cache will be removed when environment restart\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 805 µs, sys: 0 ns, total: 805 µs\n",
      "Wall time: 810 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-283b3df2-06a1-4cd6-83c5-f91cfb5371be-0', usage_metadata={'input_tokens': 11, 'output_tokens': 18, 'total_tokens': 29})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up long-term cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 0 ns, total: 17.4 ms\n",
      "Wall time: 3.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-341ffbc6-6641-42f3-a0e1-5982a07ee805-0', usage_metadata={'input_tokens': 11, 'output_tokens': 18, 'total_tokens': 29})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache.db\"))\n",
    "\n",
    "\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 ms, sys: 0 ns, total: 2.12 ms\n",
      "Wall time: 1.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-341ffbc6-6641-42f3-a0e1-5982a07ee805-0', usage_metadata={'input_tokens': 11, 'output_tokens': 18, 'total_tokens': 29})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check log probilities (every word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 28,\n",
       "  'prompt_tokens': 16,\n",
       "  'total_tokens': 44},\n",
       " 'model_name': 'gpt-4o-mini',\n",
       " 'system_fingerprint': 'fp_661538dc1f',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': {'content': [{'token': \"I'm\",\n",
       "    'bytes': [73, 39, 109],\n",
       "    'logprob': -0.04862631,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' just',\n",
       "    'bytes': [32, 106, 117, 115, 116],\n",
       "    'logprob': -0.039988343,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' a',\n",
       "    'bytes': [32, 97],\n",
       "    'logprob': -5.8603408e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' computer',\n",
       "    'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
       "    'logprob': -0.39108548,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' program',\n",
       "    'bytes': [32, 112, 114, 111, 103, 114, 97, 109],\n",
       "    'logprob': -5.669615e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ',',\n",
       "    'bytes': [44],\n",
       "    'logprob': -4.9617593e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' so',\n",
       "    'bytes': [32, 115, 111],\n",
       "    'logprob': -0.10020689,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' I',\n",
       "    'bytes': [32, 73],\n",
       "    'logprob': -1.9361265e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': \" don't\",\n",
       "    'bytes': [32, 100, 111, 110, 39, 116],\n",
       "    'logprob': -0.004078759,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' have',\n",
       "    'bytes': [32, 104, 97, 118, 101],\n",
       "    'logprob': -0.00081355876,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' feelings',\n",
       "    'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115],\n",
       "    'logprob': -1.0445127e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ',', 'bytes': [44], 'logprob': -0.0025005098, 'top_logprobs': []},\n",
       "   {'token': ' but',\n",
       "    'bytes': [32, 98, 117, 116],\n",
       "    'logprob': -3.1281633e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': \" I'm\",\n",
       "    'bytes': [32, 73, 39, 109],\n",
       "    'logprob': -0.0031446815,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' here',\n",
       "    'bytes': [32, 104, 101, 114, 101],\n",
       "    'logprob': -0.0002040172,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' and',\n",
       "    'bytes': [32, 97, 110, 100],\n",
       "    'logprob': -0.0011793931,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' ready',\n",
       "    'bytes': [32, 114, 101, 97, 100, 121],\n",
       "    'logprob': -1.5213274e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' to',\n",
       "    'bytes': [32, 116, 111],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' help',\n",
       "    'bytes': [32, 104, 101, 108, 112],\n",
       "    'logprob': -0.048587486,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' you',\n",
       "    'bytes': [32, 121, 111, 117],\n",
       "    'logprob': -0.0019324434,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '!', 'bytes': [33], 'logprob': -0.038409915, 'top_logprobs': []},\n",
       "   {'token': ' How',\n",
       "    'bytes': [32, 72, 111, 119],\n",
       "    'logprob': -1.3186812e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' can',\n",
       "    'bytes': [32, 99, 97, 110],\n",
       "    'logprob': -0.08129328,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' I', 'bytes': [32, 73], 'logprob': 0.0, 'top_logprobs': []},\n",
       "   {'token': ' assist',\n",
       "    'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "    'logprob': -9.0883464e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' you',\n",
       "    'bytes': [32, 121, 111, 117],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' today',\n",
       "    'bytes': [32, 116, 111, 100, 97, 121],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '?',\n",
       "    'bytes': [63],\n",
       "    'logprob': -1.9361265e-07,\n",
       "    'top_logprobs': []}]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True)\n",
    "\n",
    "msg = llm.invoke((\"human\", \"how are you today\"))\n",
    "\n",
    "msg.response_metadata[\"logprobs\"][\"content\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trim message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-openai\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key part is max_tokens and  token counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"I'm JIn,what do you call a speechless parrot\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strategy is \"last\"\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also include the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    include_system = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allow partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    include_system = True,\n",
    "    allow_partial = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we can chainning the trim message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+  \n",
      "| trim_messages_input |  \n",
      "+---------------------+  \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +-------------+      \n",
      "    | Lambda(...) |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +------------+       \n",
      "    | ChatOpenAI |       \n",
      "    +------------+       \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "  +------------------+   \n",
      "  | ChatOpenAIOutput |   \n",
      "  +------------------+   \n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Notice we don't pass in messages. This creates\n",
    "# a RunnableLambda that takes messages as input\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | llm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A polygon! Because it’s a bird that can’t speak! 🦜😄', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 32, 'total_tokens': 50}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-cd2de8ad-77cd-4afe-a62c-051e4508c6d6-0', usage_metadata={'input_tokens': 32, 'output_tokens': 18, 'total_tokens': 50})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+  \n",
      "| trim_messages_input |  \n",
      "+---------------------+  \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +-------------+      \n",
      "    | Lambda(...) |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +------------+       \n",
      "    | ChatOpenAI |       \n",
      "    +------------+       \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "  +------------------+   \n",
      "  | ChatOpenAIOutput |   \n",
      "  +------------------+   \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also add chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = InMemoryChatMessageHistory(messages=messages[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run c747357e-e91b-4ce0-af40-c551cfcbed72 not found for run e9a8c17c-b19c-4c8d-937d-a5745e597641. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Jin! You\\'d call a speechless parrot \"a polygon!\" Because it’s not saying anything, but it still has a lot of sides! 🦜😄', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 72, 'total_tokens': 108}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-c959e219-02d2-4afa-a154-240c2619fe4f-0', usage_metadata={'input_tokens': 72, 'output_tokens': 36, 'total_tokens': 108})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_history_message(session_id):\n",
    "    if session_id != 3 :\n",
    "        return InMemoryChatMessageHistory()\n",
    "    return chat_history\n",
    "# define trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=100,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")\n",
    "# init model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain = trimmer | model\n",
    "llm_with_history = RunnableWithMessageHistory(chain,get_history_message)\n",
    "\n",
    "\n",
    "\n",
    "llm_with_history.invoke(\n",
    "    [messages],\n",
    "    config={\"configurable\":{\"session_id\":3}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a43d1877-bba8-4d16-a249-c4cd9c8ece4b not found for run 3c590cb7-678e-45d8-b868-a5b02f7b274a. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Jin! But if you forget it, just remember: it sounds like the sound a genie makes when they grant a wish! 🧞\\u200d♂️✨', response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 84, 'total_tokens': 119}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-c400b98c-9dd8-4be1-98b2-7ee01e19239f-0', usage_metadata={'input_tokens': 84, 'output_tokens': 35, 'total_tokens': 119})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_history.invoke(\n",
    "    [(HumanMessage(\"what's my name\"))],\n",
    "    config={\"configurable\":{\"session_id\":3}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " +----------------------+  \n",
      " | _enter_history_input |  \n",
      " +----------------------+  \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| Lambda(_enter_history) | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "      +-------------+      \n",
      "      | Lambda(...) |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "   +------------------+    \n",
      "   | ChatOpenAIOutput |    \n",
      "   +------------------+    \n"
     ]
    }
   ],
   "source": [
    "llm_with_history.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
