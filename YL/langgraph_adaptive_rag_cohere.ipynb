{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  langchain langchain_cohere  langchain_openai tiktoken langchainhub chromadb langgraph \n",
    "# ! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nXpHCXykO8o6YB9A57EQpNBxZKX5ngGy1rzbv19h\n"
     ]
    }
   ],
   "source": [
    "###LLMs\n",
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "#os.environ[\"COHERE_API_KEY\"]=\"nXpHCXykO8o6YB9A57EQpNBxZKX5ngGy1rzbv19h\"\n",
    "\n",
    "cohere_api_key=os.environ[\"COHERE_API_KEY\"]\n",
    "print(cohere_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embeddings\n",
    "embd = CohereEmbeddings()\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore  矢量存储\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embd,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "\n",
    "from langchain_cohere import ChatCohere                    #ChatCohere类，用于处理自然语言\n",
    "from langchain_core.prompts import ChatPromptTemplate      #ChatPromptTemplate类，用于创建交互式提示模板\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field    #BaseModel和Field类，用于定义数据模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "\n",
    "# Preamble 前言\n",
    "preamble = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "# LLM with tool use and preamble    \n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_router = llm.bind_tools(                      #工具绑定\n",
    "    tools=[web_search, vectorstore], preamble=preamble\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#组合 route_prompt 和 structured_llm_router 来处理用户的问题\n",
    "question_router = route_prompt | structured_llm_router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I will search for who the Bears will pick in the NFL draft and relay this information to the user.' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '51c29e9e-d081-466b-bd62-545b255e0662', 'tool_calls': [{'id': 'b45a73130a55429b95b0a3fc8114cfd0', 'function': {'name': 'web_search', 'arguments': '{\"query\": \"who will the bears pick in the NFL draft\"}'}, 'type': 'function'}], 'token_count': {'input_tokens': 915, 'output_tokens': 71}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '51c29e9e-d081-466b-bd62-545b255e0662', 'tool_calls': [{'id': 'b45a73130a55429b95b0a3fc8114cfd0', 'function': {'name': 'web_search', 'arguments': '{\"query\": \"who will the bears pick in the NFL draft\"}'}, 'type': 'function'}], 'token_count': {'input_tokens': 915, 'output_tokens': 71}} id='run-35d49118-cd01-43c7-92c0-1e40f8ab4d18-0' tool_calls=[{'name': 'web_search', 'args': {'query': 'who will the bears pick in the NFL draft'}, 'id': '1f888a55047e4f93881b33328275ccd8'}]\n",
      "[{'id': 'b45a73130a55429b95b0a3fc8114cfd0', 'function': {'name': 'web_search', 'arguments': '{\"query\": \"who will the bears pick in the NFL draft\"}'}, 'type': 'function'}]\n",
      "\n",
      "content=\"I will search the vectorstore for 'agent memory' to find the answer to the question.\" additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c5e3515e-717f-4418-a504-4b0ae44ffc67', 'tool_calls': [{'id': 'bf25561058f34a38a980152d8016c680', 'function': {'name': 'vectorstore', 'arguments': '{\"query\": \"agent memory\"}'}, 'type': 'function'}], 'token_count': {'input_tokens': 912, 'output_tokens': 61}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c5e3515e-717f-4418-a504-4b0ae44ffc67', 'tool_calls': [{'id': 'bf25561058f34a38a980152d8016c680', 'function': {'name': 'vectorstore', 'arguments': '{\"query\": \"agent memory\"}'}, 'type': 'function'}], 'token_count': {'input_tokens': 912, 'output_tokens': 61}} id='run-7d7c195c-4627-4930-bf3f-c4d1aaf0a28b-0' tool_calls=[{'name': 'vectorstore', 'args': {'query': 'agent memory'}, 'id': '5c16f4ce64c846598b6cfaa844587435'}]\n",
      "[{'id': 'bf25561058f34a38a980152d8016c680', 'function': {'name': 'vectorstore', 'arguments': '{\"query\": \"agent memory\"}'}, 'type': 'function'}]\n",
      "\n",
      "content=\"Hi, I'm doing well, thank you for asking! How can I help you today?\" additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '5a3c5bc6-32e8-4f75-921a-a6ddd043fc60', 'token_count': {'input_tokens': 909, 'output_tokens': 100}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '5a3c5bc6-32e8-4f75-921a-a6ddd043fc60', 'token_count': {'input_tokens': 909, 'output_tokens': 100}} id='run-18a53321-b17e-4a3b-b8af-b140375677ba-0'\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "response = question_router.invoke(\n",
    "    {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n",
    ")\n",
    "print(response)\n",
    "print(response.response_metadata[\"tool_calls\"],end=\"\\n\\n\")            #打印工具调用元数据信息   \n",
    "\n",
    "response = question_router.invoke({\"question\": \"What are the types of agent memory?\"})\n",
    "print(response)\n",
    "print(response.response_metadata[\"tool_calls\"],end=\"\\n\\n\")\n",
    "\n",
    "response = question_router.invoke({\"question\": \"Hi how are you?\"})\n",
    "print(response)\n",
    "print(\"tool_calls\" in response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions')]\n",
      "\n",
      "question:types of agent memory\n",
      "\n",
      "document:Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "\n",
      "\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 8. Categorization of human memory.\n",
      "We can roughly consider the following mappings:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Data model \n",
    "# 文档相关性评分\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Prompt\n",
    "preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments, preamble=preamble)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),          #根据已检索文档 + 用户问题进行判断\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "question = \"types of agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content           #第二个页面的内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据问题和检索文档内容进行相关性评分\n",
    "response = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "\n",
    "print(docs,end=\"\\n\\n\")\n",
    "print(\"question:\" + question,end=\"\\n\\n\")\n",
    "print(\"document:\"+ doc_txt,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser  #解析字符串输出\n",
    "\n",
    "# Preamble 回答问题助手\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "def prompt(x):\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                f\"Question: {x['question']} \\nAnswer: \",\n",
    "                additional_kwargs={\"documents\": x[\"documents\"]},\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Chain 链路设计\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions')]\n",
      "\n",
      "question:types of agent memory\n",
      "\n",
      "<class 'str'>\n",
      "generation: Types of memory include:\n",
      "- Sensory Memory\n",
      "- Short-Term Memory (STM) or Working Memory\n",
      "- Long-Term Memory (LTM)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "\n",
    "print(docs,end=\"\\n\\n\")\n",
    "print(\"question:\" + question,end=\"\\n\\n\")\n",
    "\n",
    "print(type(generation))\n",
    "print(f\"generation:\", generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM fallback\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "\n",
    "# Prompt\n",
    "def prompt(x):\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [HumanMessage(f\"Question: {x['question']} \\nAnswer: \")]\n",
    "    )\n",
    "\n",
    "\n",
    "# Chain \n",
    "llm_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Hi how are you?\n",
      "generation: I don't have feelings as an AI chatbot, but I'm here to assist you with any questions or tasks you may have. How can I help?\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"Hi how are you?\"\n",
    "generation = llm_chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"question:\",question)\n",
    "print(f\"generation:\", generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(\n",
    "    GradeHallucinations\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "# 根据事实和LLMs生成答案进行幻觉判断\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", preamble),                                        ###修改prompt，将前言（即问题）传递给llm                                     \n",
    "        \n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#chain\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions')]\n",
      "\n",
      "generation: I don't have feelings as an AI chatbot, but I'm here to assist you with any questions or tasks you may have. How can I help?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs,end=\"\\n\\n\")\n",
    "print(f\"generation:\", generation)\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "# 根据用户问题和LLMs生成答案怕判断是否回答了问题\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#chain\n",
    "answer_grader = answer_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question：Hi how are you?\n",
      "\n",
      "generation: I don't have feelings as an AI chatbot, but I'm here to assist you with any questions or tasks you may have. How can I help?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"question：\" + question,end=\"\\n\\n\")\n",
    "print(f\"generation:\", generation)\n",
    "\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Search\n",
    "\n",
    "import os \n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "# os.environ['TAVILY_API_KEY']=\"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\"\n",
    "taily_api_ker=os.environ['TAVILY_API_KEY']\n",
    "print(taily_api_ker)\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph\n",
    "Capture the flow in as a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):             \n",
    "    \"\"\"|\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def llm_fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---LLM Fallback---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    if not isinstance(documents, list):\n",
    "        documents = [documents]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    print(docs)\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    \n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges ###\n",
    "\n",
    "# 将问题路由至网络搜索或 RAG\n",
    "def route_question(state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.    \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "\n",
    "    # Fallback to LLM or raise error if no decision\n",
    "    if \"tool_calls\" not in source.additional_kwargs:     #额外关键词\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"llm_fallback\"\n",
    "    #tool_calls 列表为空\n",
    "    if len(source.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "        raise \"Router could not decide source\"\n",
    "\n",
    "    # Choose datasource\n",
    "    datasource = source.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "    #根据工具调用信息选择数据来源(下一步操作)\n",
    "    if datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    else:\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"vectorstore\"                      ###有点奇怪 不是llm_fallback吗？\n",
    "\n",
    "#确定是否生成答案，或者重新生成问题。\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")          #正在评估已分级的文档\n",
    "\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    #不存在相关文档\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        # 所有文档都不相关于问题，需要进行网络搜索。\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    \n",
    "    #存在相关文档\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "# 确定生成是否基于文档并回答问题\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")                \n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    #幻觉打分（yes\\no）\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    \n",
    "    while not score.binary_score:\n",
    "        score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )   \n",
    "    time.sleep(6) \n",
    "\n",
    "    print(score)\n",
    "    \n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":  # 生成的内容基于文档\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")  # 生成的内容是否回答了问题\n",
    "        # 根据问题与生成内容进行评分\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "\n",
    "        if score is not None:\n",
    "            grade = score.binary_score\n",
    "            # 根据问题回答评分做出决策\n",
    "            if grade == \"yes\":\n",
    "                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                return \"useful\"\n",
    "            else:\n",
    "                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                return \"not useful\"\n",
    "        else:\n",
    "            print(\"---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\")\n",
    "            return \"not useful\"\n",
    "    else:  # 生成的内容未基于文档\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")  # 建议重新尝试\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes 和对应函数 \n",
    "workflow.add_node(\"web_search\", web_search)            # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)                # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)                # rag\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback)        # llm\n",
    "\n",
    "### Build graph ###   \n",
    "# 添加条件边缘\n",
    "workflow.add_conditional_edges(\n",
    "    START,                               # 开始节点\n",
    "    route_question,                      # 路由问题的函数\n",
    "    {\n",
    "        \"web_search\": \"web_search\",      # 如果路由函数返回 \"web_search\"，转到网络搜索节点\n",
    "        \"vectorstore\": \"retrieve\",       # 如果返回 \"vectorstore\"，转到检索节点\n",
    "        \"llm_fallback\": \"llm_fallback\",  # 如果返回 \"llm_fallback\"，转到LLM备选节点\n",
    "    },\n",
    ")\n",
    "\n",
    "# 添加普通边缘\n",
    "workflow.add_edge(\"web_search\", \"generate\")       # 网络搜索节点完成后转到生成节点\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")  # 检索节点完成后转到评分文档节点\n",
    "\n",
    "# 添加条件边缘\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",                     # 评分文档节点\n",
    "    decide_to_generate,                    # 决定是否生成的函数\n",
    "    {\n",
    "        \"web_search\": \"web_search\",        # 如果决定生成，转到网络搜索节点\n",
    "        \"generate\": \"generate\",            # 如果决定不生成，转到生成节点\n",
    "    },\n",
    ")\n",
    "\n",
    "# 添加条件边缘\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",                                      # 生成节点\n",
    "    grade_generation_v_documents_and_question,       # 评分生成文档和问题的函数\n",
    "    {\n",
    "        \"not supported\": \"generate\",                 # Hallucinations: re-generate\n",
    "        \"not useful\": \"web_search\",                  # Fails to answer question: fall-back to web-search\n",
    "        \"useful\": END,                               #生成结果有用，结束流程\n",
    "    },\n",
    ")\n",
    "\n",
    "# 添加普通边缘\n",
    "workflow.add_edge(\"llm_fallback\", END)               # LLM备选节点完成后结束流程\n",
    "\n",
    "# Compile \n",
    "# 编译状态图\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://www.sportingnews.com/us/nfl/news/nfl-mock-draft-chicago-bears-caleb-williams-rome-odunze-2024/6e380a3a762908491684f2f1', 'content': \"Chicago Bears power up offense in 7-round mock The Chicago Bears were out in full force at Caleb Williams' pro day on Wednesday, all but solidifying the expectation that general manager Ryan Poles will select the USC quarterback with the first pick in the 2024 NFL Draft.  Austin Booker, EDGE, Kansas The Bears won't leave the 2024 draft without an edge rusher, and there's even a chance a player like Dallas Turner (Alabama) will be the pick at No. 9 overall. Armed with a second top-10 pick, No. 9 overall, Poles has a rare opportunity to flip the Chicago Bears from a seven-win team to an NFC North contender in one NFL Draft class.  Round 3.75: Kiran Amegadjie, OT, Yale The Bears are showing high-level interest in Amegadjie, one of the top small-school players in the 2024 NFL Draft. In addition to his Chicago Bears coverage, Perez is a respected member of NFL Draft media and was a past winner of The Huddle's Mock Draft competition.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-2024-nfl-draft-grades-expert-picks-caleb-williams-rome-odunze/', 'content': \"Every player selected by picks the Bears traded away in 2024 NFL draft 2024 NFL draft: Experts hand out final grades for Bears Breaking down the Bears' 2024 draft picks Grading every Bears pick in the 2024 NFL draft Twitter reacts to Bears trading up for EDGE Austin Booker Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. Sections Advertisement 2024 NFL draft: Experts hand out final grades for Bears Share this article The 2024 NFL draft is officially in the books, and the Chicago Bears came out of it with some potential generational prospects and players who could develop into long-term cornerstones.  Bleacher Report: A Apr 25, 2024; Detroit, MI, USA; Washington Huskies wide receiver Rome Odunze poses after being selected by the Chicago Bears as the No. 8 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Associated Press: A+ Apr 25, 2024; Detroit, MI, USA; Southern California Trojans quarterback Caleb Williams holds up his jersey after being selected by the Chicago Bears as the No. 1 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Amegadjie could end up being the steal of this class, and based on how well GM Ryan Poles has mined the lower rungs of the draft for offensive line talent, we could see another deep class for the Bears in a critical season. \"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-caleb-williams-drake-maye-best-fit/', 'content': \"Most Popular 2024 NFL mock draft: QBs, WRs dominate top 10 in latest first-round projections Former Bears players on 2024 divisional playoff teams What the Bears are getting in new OC Shane Waldron Top 25 free agent targets for the Bears in 2024 Bears OC Shane Waldron already working on filling out his staff 40 free agent targets for Bears in divisional round playoffs Twitter reacts to Bears' expected hiring of Shane Waldron as offensive coordinator Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. There are situations where the Bears decide to pass up on picking a quarterback with the first pick, trade down to the third pick with the Patriots and secure receiver Marvin Harrison Jr., or even keep Justin Fields and take a Day 2 quarterback and develop him into their new offense.  We examine the pros and cons of each prospect: Caleb Williams USC Trojans quarterback Caleb Williams (13) during the pregame warmup before playing the Arizona State Sun Devils at Mountain America Stadium in Tempe on Sept. 23, 2023.  Share this article The Chicago Bears are sitting in a prime spot with both the No. 1 and No. 9 picks in the 2024 NFL draft.  The only benefit of taking McCarthy would be giving Fields one more chance to right the ship, adding more offensive weapons in the draft, and securing a backup plan for the future.\"}, {'url': 'https://bleacherreport.com/articles/10102981-nfl-rumors-justin-fields-has-made-bears-decision-to-draft-qb-in-2024-difficult', 'content': \"The Bears are 6-9 but are in position to select No. 1 due to last year's trade with the Carolina Panthers with USC's Caleb Williams and UNC's Drake Maye among the potential prizes at the top.  The Chicago Bears appear set up to have the No. 1 pick in the 2024 NFL draft, but their decision has reportedly been complicated by Justin Fields' strong play down the stretch.  However, the play of Fields has been improving and ESPN's Jeremy Fowler reported on SportsCenter that he is, at the very least, giving the front office plenty to think about entering the offseason.  The Bears had an opportunity to take a quarterback with the top pick in 2023, with Bryce Young and C.J. Stroud among the top options. Wins in those two could lead the Bears to an 8-9 season and put Fields in position to assume the quarterback position again in 2024.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-mock-draft-2024-senior-bowl/', 'content': \"Most Popular 6 players who stood out during the 2024 Senior Bowl Bears focus on offense with first-round picks in Matt Miller's 2024 mock draft 2.0 Every NFL team's 2024 strength of schedule, from most difficult to easiest Best Bears player to wear every jersey number Albert Breer hints at potential Justin Fields trade deadline Bears’ top 2024 draft prospects: Arkansas C Beaux Limmer (No. 14) Adam Schefter: 'Widespread consensus' that Bears will draft Caleb Williams at No. 1 Advertisement Follow BearsWire!  Sections Advertisement Full 7-round Bears 2024 mock draft 3.0: Post-Senior Bowl edition Share this article As the Chicago Bears gear up for a pivotal offseason, the team ushers in a new offense with former Seattle coordinator Shane Waldron and the first overall pick. Mandatory Credit: Melina Myers-USA TODAY Sports Strengths: Explosive off the edge, has speed and power, pass rush arsenal Why the Bears: The Florida State edge rusher complements Montez Sweat, helping Chicago’s defense continue to generate pressure and making life difficult for opposing quarterbacks. Strengths: Great size and length, excellent in zone coverage Senior Bowl: Last week, Hart put on a show with great coverage, multiple interceptions in one-on-ones, and finished the game with three tackles, including a tackle for loss.  By signing up you agree to our Privacy Policy Share this article Advertisement Want the latest news and insights on your favorite team? \"}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://www.sportingnews.com/us/nfl/news/nfl-mock-draft-chicago-bears-caleb-williams-rome-odunze-2024/6e380a3a762908491684f2f1', 'content': \"Chicago Bears power up offense in 7-round mock The Chicago Bears were out in full force at Caleb Williams' pro day on Wednesday, all but solidifying the expectation that general manager Ryan Poles will select the USC quarterback with the first pick in the 2024 NFL Draft.  Austin Booker, EDGE, Kansas The Bears won't leave the 2024 draft without an edge rusher, and there's even a chance a player like Dallas Turner (Alabama) will be the pick at No. 9 overall. Armed with a second top-10 pick, No. 9 overall, Poles has a rare opportunity to flip the Chicago Bears from a seven-win team to an NFC North contender in one NFL Draft class.  Round 3.75: Kiran Amegadjie, OT, Yale The Bears are showing high-level interest in Amegadjie, one of the top small-school players in the 2024 NFL Draft. In addition to his Chicago Bears coverage, Perez is a respected member of NFL Draft media and was a past winner of The Huddle's Mock Draft competition.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-2024-nfl-draft-grades-expert-picks-caleb-williams-rome-odunze/', 'content': \"Every player selected by picks the Bears traded away in 2024 NFL draft 2024 NFL draft: Experts hand out final grades for Bears Breaking down the Bears' 2024 draft picks Grading every Bears pick in the 2024 NFL draft Twitter reacts to Bears trading up for EDGE Austin Booker Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. Sections Advertisement 2024 NFL draft: Experts hand out final grades for Bears Share this article The 2024 NFL draft is officially in the books, and the Chicago Bears came out of it with some potential generational prospects and players who could develop into long-term cornerstones.  Bleacher Report: A Apr 25, 2024; Detroit, MI, USA; Washington Huskies wide receiver Rome Odunze poses after being selected by the Chicago Bears as the No. 8 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Associated Press: A+ Apr 25, 2024; Detroit, MI, USA; Southern California Trojans quarterback Caleb Williams holds up his jersey after being selected by the Chicago Bears as the No. 1 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Amegadjie could end up being the steal of this class, and based on how well GM Ryan Poles has mined the lower rungs of the draft for offensive line talent, we could see another deep class for the Bears in a critical season. \"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-caleb-williams-drake-maye-best-fit/', 'content': \"Most Popular 2024 NFL mock draft: QBs, WRs dominate top 10 in latest first-round projections Former Bears players on 2024 divisional playoff teams What the Bears are getting in new OC Shane Waldron Top 25 free agent targets for the Bears in 2024 Bears OC Shane Waldron already working on filling out his staff 40 free agent targets for Bears in divisional round playoffs Twitter reacts to Bears' expected hiring of Shane Waldron as offensive coordinator Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. There are situations where the Bears decide to pass up on picking a quarterback with the first pick, trade down to the third pick with the Patriots and secure receiver Marvin Harrison Jr., or even keep Justin Fields and take a Day 2 quarterback and develop him into their new offense.  We examine the pros and cons of each prospect: Caleb Williams USC Trojans quarterback Caleb Williams (13) during the pregame warmup before playing the Arizona State Sun Devils at Mountain America Stadium in Tempe on Sept. 23, 2023.  Share this article The Chicago Bears are sitting in a prime spot with both the No. 1 and No. 9 picks in the 2024 NFL draft.  The only benefit of taking McCarthy would be giving Fields one more chance to right the ship, adding more offensive weapons in the draft, and securing a backup plan for the future.\"}, {'url': 'https://bleacherreport.com/articles/10102981-nfl-rumors-justin-fields-has-made-bears-decision-to-draft-qb-in-2024-difficult', 'content': \"The Bears are 6-9 but are in position to select No. 1 due to last year's trade with the Carolina Panthers with USC's Caleb Williams and UNC's Drake Maye among the potential prizes at the top.  The Chicago Bears appear set up to have the No. 1 pick in the 2024 NFL draft, but their decision has reportedly been complicated by Justin Fields' strong play down the stretch.  However, the play of Fields has been improving and ESPN's Jeremy Fowler reported on SportsCenter that he is, at the very least, giving the front office plenty to think about entering the offseason.  The Bears had an opportunity to take a quarterback with the top pick in 2023, with Bryce Young and C.J. Stroud among the top options. Wins in those two could lead the Bears to an 8-9 season and put Fields in position to assume the quarterback position again in 2024.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-mock-draft-2024-senior-bowl/', 'content': \"Most Popular 6 players who stood out during the 2024 Senior Bowl Bears focus on offense with first-round picks in Matt Miller's 2024 mock draft 2.0 Every NFL team's 2024 strength of schedule, from most difficult to easiest Best Bears player to wear every jersey number Albert Breer hints at potential Justin Fields trade deadline Bears’ top 2024 draft prospects: Arkansas C Beaux Limmer (No. 14) Adam Schefter: 'Widespread consensus' that Bears will draft Caleb Williams at No. 1 Advertisement Follow BearsWire!  Sections Advertisement Full 7-round Bears 2024 mock draft 3.0: Post-Senior Bowl edition Share this article As the Chicago Bears gear up for a pivotal offseason, the team ushers in a new offense with former Seattle coordinator Shane Waldron and the first overall pick. Mandatory Credit: Melina Myers-USA TODAY Sports Strengths: Explosive off the edge, has speed and power, pass rush arsenal Why the Bears: The Florida State edge rusher complements Montez Sweat, helping Chicago’s defense continue to generate pressure and making life difficult for opposing quarterbacks. Strengths: Great size and length, excellent in zone coverage Senior Bowl: Last week, Hart put on a show with great coverage, multiple interceptions in one-on-ones, and finished the game with three tackles, including a tackle for loss.  By signing up you agree to our Privacy Policy Share this article Advertisement Want the latest news and insights on your favorite team? \"}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-winners-first-round-expert-analysis-caleb-williams-rome-odunze/', 'content': 'The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections — quarterback Caleb Williams (No. 1 ...'}, {'url': 'https://www.nfl.com/news/chicago-bears-2024-nfl-draft-guide-what-you-need-to-know', 'content': 'Chicago Bears Draft Overview. 2023 record: 7-10. Fourth in NFC North; missed playoffs. Bears 2024 draft picks (4): Round 1, pick 1 (from the Panthers in the D.J. Moore/2023 No. 1 overall pick ...'}, {'url': 'https://www.pff.com/news/draft-chicago-bears-2024-nfl-draft-picks-analysis-and-prospect-spotlight', 'content': \"The 2024 NFL Draft is now in the rearview mirror. After a flurry of selections from April 25 to April 27, 257 players were selected to join the NFL. With that, we give you our full recap of the Chicago Bears' draft, with analysis on every selection the team made during the weekend and an in-depth look at their top pick.. For more information on the players your favorite team drafted, it's ...\"}, {'url': 'https://www.chicagobears.com/news/2024-chicago-bears-nfl-draft-picks-news-photos-highlights', 'content': 'The Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.'}, {'url': 'https://www.nbcsportschicago.com/nfl/chicago-bears/full-2024-nfl-draft-order-for-first-round-and-how-it-affects-bears/553460/', 'content': \"The Bears have the No. 1 overall pick in this year's draft, and their all but assured decision to select Caleb Williams with that pick will be the most impactful decision of the offseason. But GM Ryan Poles won't be done there. Just a bit later, the Bears are slated to be back on the clock at No. 9, where the team has the opportunity to add another franchise cornerstone-caliber player.\"}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://www.sportingnews.com/us/nfl/news/nfl-mock-draft-chicago-bears-caleb-williams-rome-odunze-2024/6e380a3a762908491684f2f1', 'content': \"Chicago Bears power up offense in 7-round mock The Chicago Bears were out in full force at Caleb Williams' pro day on Wednesday, all but solidifying the expectation that general manager Ryan Poles will select the USC quarterback with the first pick in the 2024 NFL Draft.  Austin Booker, EDGE, Kansas The Bears won't leave the 2024 draft without an edge rusher, and there's even a chance a player like Dallas Turner (Alabama) will be the pick at No. 9 overall. Armed with a second top-10 pick, No. 9 overall, Poles has a rare opportunity to flip the Chicago Bears from a seven-win team to an NFC North contender in one NFL Draft class.  Round 3.75: Kiran Amegadjie, OT, Yale The Bears are showing high-level interest in Amegadjie, one of the top small-school players in the 2024 NFL Draft. In addition to his Chicago Bears coverage, Perez is a respected member of NFL Draft media and was a past winner of The Huddle's Mock Draft competition.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-2024-nfl-draft-grades-expert-picks-caleb-williams-rome-odunze/', 'content': \"Every player selected by picks the Bears traded away in 2024 NFL draft 2024 NFL draft: Experts hand out final grades for Bears Breaking down the Bears' 2024 draft picks Grading every Bears pick in the 2024 NFL draft Twitter reacts to Bears trading up for EDGE Austin Booker Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. Sections Advertisement 2024 NFL draft: Experts hand out final grades for Bears Share this article The 2024 NFL draft is officially in the books, and the Chicago Bears came out of it with some potential generational prospects and players who could develop into long-term cornerstones.  Bleacher Report: A Apr 25, 2024; Detroit, MI, USA; Washington Huskies wide receiver Rome Odunze poses after being selected by the Chicago Bears as the No. 8 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Associated Press: A+ Apr 25, 2024; Detroit, MI, USA; Southern California Trojans quarterback Caleb Williams holds up his jersey after being selected by the Chicago Bears as the No. 1 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Amegadjie could end up being the steal of this class, and based on how well GM Ryan Poles has mined the lower rungs of the draft for offensive line talent, we could see another deep class for the Bears in a critical season. \"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-caleb-williams-drake-maye-best-fit/', 'content': \"Most Popular 2024 NFL mock draft: QBs, WRs dominate top 10 in latest first-round projections Former Bears players on 2024 divisional playoff teams What the Bears are getting in new OC Shane Waldron Top 25 free agent targets for the Bears in 2024 Bears OC Shane Waldron already working on filling out his staff 40 free agent targets for Bears in divisional round playoffs Twitter reacts to Bears' expected hiring of Shane Waldron as offensive coordinator Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. There are situations where the Bears decide to pass up on picking a quarterback with the first pick, trade down to the third pick with the Patriots and secure receiver Marvin Harrison Jr., or even keep Justin Fields and take a Day 2 quarterback and develop him into their new offense.  We examine the pros and cons of each prospect: Caleb Williams USC Trojans quarterback Caleb Williams (13) during the pregame warmup before playing the Arizona State Sun Devils at Mountain America Stadium in Tempe on Sept. 23, 2023.  Share this article The Chicago Bears are sitting in a prime spot with both the No. 1 and No. 9 picks in the 2024 NFL draft.  The only benefit of taking McCarthy would be giving Fields one more chance to right the ship, adding more offensive weapons in the draft, and securing a backup plan for the future.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-mock-draft-2024-senior-bowl/', 'content': \"Most Popular 6 players who stood out during the 2024 Senior Bowl Bears focus on offense with first-round picks in Matt Miller's 2024 mock draft 2.0 Every NFL team's 2024 strength of schedule, from most difficult to easiest Best Bears player to wear every jersey number Albert Breer hints at potential Justin Fields trade deadline Bears’ top 2024 draft prospects: Arkansas C Beaux Limmer (No. 14) Adam Schefter: 'Widespread consensus' that Bears will draft Caleb Williams at No. 1 Advertisement Follow BearsWire!  Sections Advertisement Full 7-round Bears 2024 mock draft 3.0: Post-Senior Bowl edition Share this article As the Chicago Bears gear up for a pivotal offseason, the team ushers in a new offense with former Seattle coordinator Shane Waldron and the first overall pick. Mandatory Credit: Melina Myers-USA TODAY Sports Strengths: Explosive off the edge, has speed and power, pass rush arsenal Why the Bears: The Florida State edge rusher complements Montez Sweat, helping Chicago’s defense continue to generate pressure and making life difficult for opposing quarterbacks. Strengths: Great size and length, excellent in zone coverage Senior Bowl: Last week, Hart put on a show with great coverage, multiple interceptions in one-on-ones, and finished the game with three tackles, including a tackle for loss.  By signing up you agree to our Privacy Policy Share this article Advertisement Want the latest news and insights on your favorite team? \"}, {'url': 'https://bearswire.usatoday.com/lists/bears-2024-nfl-mock-draft-roundup-caleb-williams-or-bust/', 'content': \"Sections Advertisement 2024 NFL mock draft roundup: Caleb Williams is the Bears' pick at No. 1 Share this article The Chicago Bears have the first overall pick in the NFL draft for the second consecutive year. Picking the best players from 2023 Bears’ top 2024 draft prospects: Michigan WR Roman Wilson (No. 15) Top 25 free agent targets for the Bears in 2024 Bears hire Jason Houghtaling as assistant offensive line coach Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. Round 1, Pick 1 (from CAR) – QB Caleb Williams, USC This is going to get sticky at No. 1 if the Bears go long-term with Justin Fields, but Edwards expects Chicago to go with the expectation that Calebs Williams is the potential franchise savior of sorts. Round 1, Pick 1 (from CAR) – QB Caleb Williams, USC The Bears have a big decision to make on whether to keep Justin Fields and trade this pick or to trade Fields and grab one of the top quarterbacks in this class. NBC Sports Chicago’s Glynn Morgan has the Bears trading back one spot with the Commanders in exchange for the No. 1 pick, a couple of third-round picks (67th and 100th), and a second-round selection in 2025. \"}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://www.sportingnews.com/us/nfl/news/nfl-mock-draft-chicago-bears-caleb-williams-rome-odunze-2024/6e380a3a762908491684f2f1', 'content': \"Chicago Bears power up offense in 7-round mock The Chicago Bears were out in full force at Caleb Williams' pro day on Wednesday, all but solidifying the expectation that general manager Ryan Poles will select the USC quarterback with the first pick in the 2024 NFL Draft.  Austin Booker, EDGE, Kansas The Bears won't leave the 2024 draft without an edge rusher, and there's even a chance a player like Dallas Turner (Alabama) will be the pick at No. 9 overall. Armed with a second top-10 pick, No. 9 overall, Poles has a rare opportunity to flip the Chicago Bears from a seven-win team to an NFC North contender in one NFL Draft class.  Round 3.75: Kiran Amegadjie, OT, Yale The Bears are showing high-level interest in Amegadjie, one of the top small-school players in the 2024 NFL Draft. In addition to his Chicago Bears coverage, Perez is a respected member of NFL Draft media and was a past winner of The Huddle's Mock Draft competition.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-2024-nfl-draft-grades-expert-picks-caleb-williams-rome-odunze/', 'content': \"Every player selected by picks the Bears traded away in 2024 NFL draft 2024 NFL draft: Experts hand out final grades for Bears Breaking down the Bears' 2024 draft picks Grading every Bears pick in the 2024 NFL draft Twitter reacts to Bears trading up for EDGE Austin Booker Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. Sections Advertisement 2024 NFL draft: Experts hand out final grades for Bears Share this article The 2024 NFL draft is officially in the books, and the Chicago Bears came out of it with some potential generational prospects and players who could develop into long-term cornerstones.  Bleacher Report: A Apr 25, 2024; Detroit, MI, USA; Washington Huskies wide receiver Rome Odunze poses after being selected by the Chicago Bears as the No. 8 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Associated Press: A+ Apr 25, 2024; Detroit, MI, USA; Southern California Trojans quarterback Caleb Williams holds up his jersey after being selected by the Chicago Bears as the No. 1 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Amegadjie could end up being the steal of this class, and based on how well GM Ryan Poles has mined the lower rungs of the draft for offensive line talent, we could see another deep class for the Bears in a critical season. \"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-caleb-williams-drake-maye-best-fit/', 'content': \"Most Popular 2024 NFL mock draft: QBs, WRs dominate top 10 in latest first-round projections Former Bears players on 2024 divisional playoff teams What the Bears are getting in new OC Shane Waldron Top 25 free agent targets for the Bears in 2024 Bears OC Shane Waldron already working on filling out his staff 40 free agent targets for Bears in divisional round playoffs Twitter reacts to Bears' expected hiring of Shane Waldron as offensive coordinator Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. There are situations where the Bears decide to pass up on picking a quarterback with the first pick, trade down to the third pick with the Patriots and secure receiver Marvin Harrison Jr., or even keep Justin Fields and take a Day 2 quarterback and develop him into their new offense.  We examine the pros and cons of each prospect: Caleb Williams USC Trojans quarterback Caleb Williams (13) during the pregame warmup before playing the Arizona State Sun Devils at Mountain America Stadium in Tempe on Sept. 23, 2023.  Share this article The Chicago Bears are sitting in a prime spot with both the No. 1 and No. 9 picks in the 2024 NFL draft.  The only benefit of taking McCarthy would be giving Fields one more chance to right the ship, adding more offensive weapons in the draft, and securing a backup plan for the future.\"}, {'url': 'https://bleacherreport.com/articles/10102981-nfl-rumors-justin-fields-has-made-bears-decision-to-draft-qb-in-2024-difficult', 'content': \"The Bears are 6-9 but are in position to select No. 1 due to last year's trade with the Carolina Panthers with USC's Caleb Williams and UNC's Drake Maye among the potential prizes at the top.  The Chicago Bears appear set up to have the No. 1 pick in the 2024 NFL draft, but their decision has reportedly been complicated by Justin Fields' strong play down the stretch.  However, the play of Fields has been improving and ESPN's Jeremy Fowler reported on SportsCenter that he is, at the very least, giving the front office plenty to think about entering the offseason.  The Bears had an opportunity to take a quarterback with the top pick in 2023, with Bryce Young and C.J. Stroud among the top options. Wins in those two could lead the Bears to an 8-9 season and put Fields in position to assume the quarterback position again in 2024.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-mock-draft-2024-senior-bowl/', 'content': \"Most Popular 6 players who stood out during the 2024 Senior Bowl Bears focus on offense with first-round picks in Matt Miller's 2024 mock draft 2.0 Every NFL team's 2024 strength of schedule, from most difficult to easiest Best Bears player to wear every jersey number Albert Breer hints at potential Justin Fields trade deadline Bears’ top 2024 draft prospects: Arkansas C Beaux Limmer (No. 14) Adam Schefter: 'Widespread consensus' that Bears will draft Caleb Williams at No. 1 Advertisement Follow BearsWire!  Sections Advertisement Full 7-round Bears 2024 mock draft 3.0: Post-Senior Bowl edition Share this article As the Chicago Bears gear up for a pivotal offseason, the team ushers in a new offense with former Seattle coordinator Shane Waldron and the first overall pick. Mandatory Credit: Melina Myers-USA TODAY Sports Strengths: Explosive off the edge, has speed and power, pass rush arsenal Why the Bears: The Florida State edge rusher complements Montez Sweat, helping Chicago’s defense continue to generate pressure and making life difficult for opposing quarterbacks. Strengths: Great size and length, excellent in zone coverage Senior Bowl: Last week, Hart put on a show with great coverage, multiple interceptions in one-on-ones, and finished the game with three tackles, including a tackle for loss.  By signing up you agree to our Privacy Policy Share this article Advertisement Want the latest news and insights on your favorite team? \"}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://www.sportingnews.com/us/nfl/news/nfl-mock-draft-chicago-bears-caleb-williams-rome-odunze-2024/6e380a3a762908491684f2f1', 'content': \"Chicago Bears power up offense in 7-round mock The Chicago Bears were out in full force at Caleb Williams' pro day on Wednesday, all but solidifying the expectation that general manager Ryan Poles will select the USC quarterback with the first pick in the 2024 NFL Draft.  Austin Booker, EDGE, Kansas The Bears won't leave the 2024 draft without an edge rusher, and there's even a chance a player like Dallas Turner (Alabama) will be the pick at No. 9 overall. Armed with a second top-10 pick, No. 9 overall, Poles has a rare opportunity to flip the Chicago Bears from a seven-win team to an NFC North contender in one NFL Draft class.  Round 3.75: Kiran Amegadjie, OT, Yale The Bears are showing high-level interest in Amegadjie, one of the top small-school players in the 2024 NFL Draft. In addition to his Chicago Bears coverage, Perez is a respected member of NFL Draft media and was a past winner of The Huddle's Mock Draft competition.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-2024-nfl-draft-grades-expert-picks-caleb-williams-rome-odunze/', 'content': \"Every player selected by picks the Bears traded away in 2024 NFL draft 2024 NFL draft: Experts hand out final grades for Bears Breaking down the Bears' 2024 draft picks Grading every Bears pick in the 2024 NFL draft Twitter reacts to Bears trading up for EDGE Austin Booker Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. Sections Advertisement 2024 NFL draft: Experts hand out final grades for Bears Share this article The 2024 NFL draft is officially in the books, and the Chicago Bears came out of it with some potential generational prospects and players who could develop into long-term cornerstones.  Bleacher Report: A Apr 25, 2024; Detroit, MI, USA; Washington Huskies wide receiver Rome Odunze poses after being selected by the Chicago Bears as the No. 8 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Associated Press: A+ Apr 25, 2024; Detroit, MI, USA; Southern California Trojans quarterback Caleb Williams holds up his jersey after being selected by the Chicago Bears as the No. 1 pick in the first round of the 2024 NFL Draft at Campus Martius Park and Hart Plaza. Amegadjie could end up being the steal of this class, and based on how well GM Ryan Poles has mined the lower rungs of the draft for offensive line talent, we could see another deep class for the Bears in a critical season. \"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-nfl-draft-caleb-williams-drake-maye-best-fit/', 'content': \"Most Popular 2024 NFL mock draft: QBs, WRs dominate top 10 in latest first-round projections Former Bears players on 2024 divisional playoff teams What the Bears are getting in new OC Shane Waldron Top 25 free agent targets for the Bears in 2024 Bears OC Shane Waldron already working on filling out his staff 40 free agent targets for Bears in divisional round playoffs Twitter reacts to Bears' expected hiring of Shane Waldron as offensive coordinator Advertisement Follow BearsWire! Sign up for our newsletter to get updates to your inbox, and also receive offers from us, our affiliates and partners. There are situations where the Bears decide to pass up on picking a quarterback with the first pick, trade down to the third pick with the Patriots and secure receiver Marvin Harrison Jr., or even keep Justin Fields and take a Day 2 quarterback and develop him into their new offense.  We examine the pros and cons of each prospect: Caleb Williams USC Trojans quarterback Caleb Williams (13) during the pregame warmup before playing the Arizona State Sun Devils at Mountain America Stadium in Tempe on Sept. 23, 2023.  Share this article The Chicago Bears are sitting in a prime spot with both the No. 1 and No. 9 picks in the 2024 NFL draft.  The only benefit of taking McCarthy would be giving Fields one more chance to right the ship, adding more offensive weapons in the draft, and securing a backup plan for the future.\"}, {'url': 'https://bleacherreport.com/articles/10102981-nfl-rumors-justin-fields-has-made-bears-decision-to-draft-qb-in-2024-difficult', 'content': \"The Bears are 6-9 but are in position to select No. 1 due to last year's trade with the Carolina Panthers with USC's Caleb Williams and UNC's Drake Maye among the potential prizes at the top.  The Chicago Bears appear set up to have the No. 1 pick in the 2024 NFL draft, but their decision has reportedly been complicated by Justin Fields' strong play down the stretch.  However, the play of Fields has been improving and ESPN's Jeremy Fowler reported on SportsCenter that he is, at the very least, giving the front office plenty to think about entering the offseason.  The Bears had an opportunity to take a quarterback with the top pick in 2023, with Bryce Young and C.J. Stroud among the top options. Wins in those two could lead the Bears to an 8-9 season and put Fields in position to assume the quarterback position again in 2024.\"}, {'url': 'https://bearswire.usatoday.com/lists/chicago-bears-mock-draft-2024-senior-bowl/', 'content': \"Most Popular 6 players who stood out during the 2024 Senior Bowl Bears focus on offense with first-round picks in Matt Miller's 2024 mock draft 2.0 Every NFL team's 2024 strength of schedule, from most difficult to easiest Best Bears player to wear every jersey number Albert Breer hints at potential Justin Fields trade deadline Bears’ top 2024 draft prospects: Arkansas C Beaux Limmer (No. 14) Adam Schefter: 'Widespread consensus' that Bears will draft Caleb Williams at No. 1 Advertisement Follow BearsWire!  Sections Advertisement Full 7-round Bears 2024 mock draft 3.0: Post-Senior Bowl edition Share this article As the Chicago Bears gear up for a pivotal offseason, the team ushers in a new offense with former Seattle coordinator Shane Waldron and the first overall pick. Mandatory Credit: Melina Myers-USA TODAY Sports Strengths: Explosive off the edge, has speed and power, pass rush arsenal Why the Bears: The Florida State edge rusher complements Montez Sweat, helping Chicago’s defense continue to generate pressure and making life difficult for opposing quarterbacks. Strengths: Great size and length, excellent in zone coverage Senior Bowl: Last week, Hart put on a show with great coverage, multiple interceptions in one-on-ones, and finished the game with three tackles, including a tackle for loss.  By signing up you agree to our Privacy Policy Share this article Advertisement Want the latest news and insights on your favorite team? \"}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat player are the Bears expected to draft first in the 2024 NFL draft?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 遍历状态图的输出结果\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Node 打印节点的信息\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\llm\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1133\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout hitting a stop condition. You can increase the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit by setting the `recursion_limit` config key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1137\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(read_channels(channels, output_keys))\n",
      "\u001b[1;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"What player are the Bears expected to draft first in the 2024 NFL draft?\"\n",
    "}\n",
    "\n",
    "# 遍历状态图的输出结果\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node 打印节点的信息\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # 可选：打印每个节点的完整状态\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "time.sleep(6)  # 增加 6 秒的延迟，确保不会超过 API 的调用限制\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "https://smith.langchain.com/public/623da7bb-84a7-4e53-a63e-7ccd77fb9be5/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---SCORE OBJECT IS NONE, UNABLE TO GRADE ANSWER---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[{'url': 'https://www.forbes.com/sites/timothypapandreou/2024/04/27/unlocking-productivity-why-businesses-need-ai-agents-now/?ss=transportation', 'content': \"The Major Players in the Autonomous Agent Space While Alphabet/Google and OpenAI are major players and clear leaders in the generative AI space, there are several companies that I have been looking at that are emerging with autonomous agents, each with a unique approach: Choosing the Right Platform Now before you head out to choose an autonomous agent platform, it's important to consider your company’s specific needs, where you are at in our digital transformation journey above and beyond customer service chatbots. The integration of autonomous agents signifies a significant shift in the way businesses operate The Future of Living and Working with Autonomous Agents  From my experience advising in this AI space, here are some important factors to weigh: By understanding your team’s requirements and researching these factors, you can pilot and select the most relevant autonomous agent platform to begin your AI agent journey.  A personal autonomous travel agent, for example, could handle everything from booking your flights and hotels to researching activities and attractions, utilizing your loyalty points and maximizing your credit card rewards, and even planning transportation and tours throughout your trip. More From Forbes Beyond Chatbots: Why Businesses Need AI Agents To Stay Competitive Beyond chatbots- Autonomous agents are powerful AI systems capable of independent planning and ...\"}, {'url': 'https://www.theguardian.com/books/2024/mar/25/the-big-idea-why-am-i-so-forgetful', 'content': 'In the UK, Demis Hassabis (who went on to co-found the AI company DeepMind) and Eleanor Maguire published studies of patients with an impoverished ability to imagine events, and others reported a stunning degree of overlap in the brain networks that are active during remembering and during imagining the future. Findings from my lab and others have shown that, at the moment of remembering, the brain appears to revert a bit to the state that it was in at the time, enabling us to relive these past experiences. Endel Tulving, the pioneering cognitive psychologist who coined the term, described episodic memory as the uniquely human capability for “mental time travel, roaming at will over what has happened as readily as over what might happen, independently of the physical laws that govern the universe”.  Think of all the passing encounters with people you will never see again, the times you spend waiting in a queue at the supermarket, and those awkward times when you find yourself looking at the floor while stuck in a crowded elevator. On average, episodic memory gets worse as we get older, and that is due, at least in part, to the strange developmental trajectory of the prefrontal cortex – an area of the brain that helps support episodic memory.'}, {'url': 'https://www.nature.com/articles/s41599-024-03125-y', 'content': 'For others, intentionality is defined by five elements: (i) a desire for a result, (ii) a belief about the action leading to that result, (iii) an intention to carry out the action, (iv) awareness of the accomplishment of the intention while executing the action, and (v) the ability to execute the action (Malle and Knobe, 1997).  Advertisement Exploration of the creative processes in animals, robots, and AI: who holds the authorship? Humanities and Social Sciences Communications volume\\xa011, Article\\xa0number:\\xa0611 (2024) Cite this article Metrics details Subjects Abstract Picture a simple scenario: a worm, in its modest way, traces a trail of paint as it moves across a sheet of paper. This perceptual deficiency and its consequences on the audience’s experience have led the Human–Computer Interaction (HCI) community to propose new evaluation criteria for digital devices (Berthaut et al., 2013; Berthaut et al., 2015; Bin 2018; see an extended review in Capra, 2020), including the Association (Capra et al., 2020), which designates the capacity of a device to expose to the audience the respective contributions of artists and machines in electronic performances. In the series episode ‘Author, Author’, as it is the question of personhood that is at stake through the issue of authorisation, the trial’s witnesses emphasise the importance of the Doctor’s experiences, by highlighting his ability to evolve beyond his programming, to think for himself, and even, to disobey orders. As regarding animals, this very specific issue depends on many factors such as the nature of the agents involved (weak AI or strong AIFootnote 5), the degree of intentionality of the creation (intentional or accidental), and the content or nature of the graphical creation itself (abstract or representational) (Mikalonytė and Kneer, 2022).'}, {'url': 'https://www.nature.com/articles/s41562-023-01799-z', 'content': 'Author information Authors and Affiliations UCL Institute of Cognitive Neuroscience, University College London, London, UK Eleanor Spens\\xa0&\\xa0Neil Burgess UCL Queen Square Institute of Neurology, University College London, London, UK Neil Burgess You can also search for this author in PubMed\\xa0Google Scholar You can also search for this author in PubMed\\xa0Google Scholar Contributions E.S. and N.B. designed the research and wrote the paper. In the model, the semantic form of a consolidated memory survives damage to the HF due to latent variable representations in the mPFC or the alTL (even if those in the EC are lesioned); Fig. 3a demonstrates how semantic recall performance improves with the age of a memory, reflecting the temporal gradient of retrograde amnesia (see section ‘Modelling semantic memory’). The zoom level is the ratio of the central object size in the output image to the size in the original image, given as a percentage; for example, an image with a zoom level of 80% or a ratio of 0.8 was produced by adding a margin so that the object size is 80% of the original size. Extended model The extended model was designed to capture the fact that memory traces in the hippocampus bind together a mixture of sensory and conceptual elements, with the latter encoded by concept cells61, and the fact that schemas shape the reconstruction of memories even before consolidation, as shown by the rapid onset of schema-based distortions93,94.  Our model draws together existing ideas in machine learning to suggest an explanation for the following key features of memory, only subsets of which are captured by previous models: The initial encoding of memory requires only a single exposure to the event and depends on the HF, while the consolidated form of memory is acquired more gradually2,3,10, as in the complementary learning systems (CLS) model4. '}, {'url': 'https://www.ft.com/content/561f4509-3e80-48a7-b72c-aaf4733f03aa', 'content': 'How does this fit in to the holism-reductionism debate in psychology? Danny Galbraith, St Bartholomew’s School Promoted Content Follow the topics in this article Comments Useful links Support Legal & Privacy Services Tools Community & Events More from the FT Group International Edition Top sections FT recommends Specification: Memory, Biological approaches to psychology, Research methods, Free will — determinism, Reductionism-holism Click the link below to read the article and then answer the questions: New studies find benefits in very early drug treatment of Alzheimer’s Alzheimer’s Disease affects a person’s neurons. Psychology class: New studies find benefits in very early drug treatment of Alzheimer’s Roula Khalaf, Editor of the FT, selects her favourite stories in this weekly newsletter.  Sketch and label a neuron Like plasticity, Alzheimer’s involves changes in the brain’s structure and function. Why is this a necessary part of drug research Outline examples of ‘brain scanning’ methods.'}]\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='no'\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "binary_score='yes'\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'Episodic memory, which allows us to relive past experiences.'\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "time.sleep(6)  # 增加 6 秒的延迟，确保不会超过 API 的调用限制\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "https://smith.langchain.com/public/57f3973b-6879-4fbe-ae31-9ae524c3a697/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO LLM---\n",
      "---LLM Fallback---\n",
      "\"Node 'llm_fallback':\"\n",
      "'\\n---\\n'\n",
      "('I am an AI assistant, and I do not have feelings or emotions. I am '\n",
      " 'functioning as intended and ready to assist you with your queries. How can I '\n",
      " 'help you today?')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"Hello, how are you today?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "time.sleep(6)  # 增加 6 秒的延迟，确保不会超过 API 的调用限制\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace:\n",
    "https://smith.langchain.com/public/1f628ee4-8d2d-451e-aeb1-5d5e0ede2b4f/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
